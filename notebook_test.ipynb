{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d2cb1531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "from utils.data_reader import read_IHDP_data, read_SIPP_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d06900c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "20ee2fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                                  model: tarnet                                 \n",
      "                                dataset: IHDP                                   \n",
      "                          num_units_rep: 200                                    \n",
      "                         num_units_hypo: 100                                    \n",
      "                                   actv: elu                                    \n",
      "                            kernel_init: RandomNormal                           \n",
      "                             kernel_reg: L2                                     \n",
      "                              reg_param: 0.01                                   \n",
      "                                     lr: 1e-05                                  \n",
      "                               momentum: 0.9                                    \n",
      "                               nesterov: 1                                      \n",
      "                              val_split: 0.2                                    \n",
      "                             batch_size: 64                                     \n",
      "                              num_epoch: 300                                    \n",
      "                                verbose: 1                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                                  model: tarnet                                 \n",
      "                                dataset: IHDP                                   \n",
      "                          num_units_rep: 200                                    \n",
      "                         num_units_hypo: 100                                    \n",
      "                                   actv: elu                                    \n",
      "                            kernel_init: RandomNormal                           \n",
      "                             kernel_reg: L2                                     \n",
      "                              reg_param: 0.01                                   \n",
      "                                     lr: 1e-05                                  \n",
      "                               momentum: 0.9                                    \n",
      "                               nesterov: 1                                      \n",
      "                              val_split: 0.2                                    \n",
      "                             batch_size: 64                                     \n",
      "                              num_epoch: 300                                    \n",
      "                                verbose: 1                                      \n",
      "================================================================================\n",
      "DATASET USED IHDP\n",
      "2022-05-05 14:20:40.974105: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "MODEL USED tarnet\n",
      "Epoch 1/300\n",
      "10/10 [==============================] - 2s 60ms/step - loss: 61.1780 - MSE_Loss: 54.0575 - val_loss: 63.3823 - val_MSE_Loss: 52.5060 - lr: 1.0000e-05\n",
      "Epoch 2/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 48.0057 - MSE_Loss: 41.1840 - val_loss: 52.1861 - val_MSE_Loss: 42.7394 - lr: 1.0000e-05\n",
      "Epoch 3/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 38.8268 - MSE_Loss: 32.3322 - val_loss: 43.4496 - val_MSE_Loss: 35.0132 - lr: 1.0000e-05\n",
      "Epoch 4/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 31.8930 - MSE_Loss: 26.0675 - val_loss: 37.2404 - val_MSE_Loss: 29.4558 - lr: 1.0000e-05\n",
      "Epoch 5/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 28.2236 - MSE_Loss: 22.4226 - val_loss: 33.0692 - val_MSE_Loss: 25.7019 - lr: 1.0000e-05\n",
      "Epoch 6/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 25.7186 - MSE_Loss: 19.9010 - val_loss: 30.1875 - val_MSE_Loss: 23.1337 - lr: 1.0000e-05\n",
      "Epoch 7/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 24.1001 - MSE_Loss: 18.2944 - val_loss: 28.0738 - val_MSE_Loss: 21.2577 - lr: 1.0000e-05\n",
      "Epoch 8/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 22.7124 - MSE_Loss: 17.0932 - val_loss: 26.7981 - val_MSE_Loss: 20.1595 - lr: 1.0000e-05\n",
      "Epoch 9/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 21.8938 - MSE_Loss: 16.4230 - val_loss: 25.8841 - val_MSE_Loss: 19.3517 - lr: 1.0000e-05\n",
      "Epoch 10/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 21.2657 - MSE_Loss: 15.9923 - val_loss: 25.2976 - val_MSE_Loss: 18.8196 - lr: 1.0000e-05\n",
      "Epoch 11/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 20.9516 - MSE_Loss: 15.6286 - val_loss: 24.8411 - val_MSE_Loss: 18.4015 - lr: 1.0000e-05\n",
      "Epoch 12/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 20.8074 - MSE_Loss: 15.2594 - val_loss: 24.5228 - val_MSE_Loss: 18.0928 - lr: 1.0000e-05\n",
      "Epoch 13/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 20.4839 - MSE_Loss: 15.0719 - val_loss: 24.2702 - val_MSE_Loss: 17.8626 - lr: 1.0000e-05\n",
      "Epoch 14/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 20.3130 - MSE_Loss: 14.7321 - val_loss: 23.9885 - val_MSE_Loss: 17.5967 - lr: 1.0000e-05\n",
      "Epoch 15/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 19.8303 - MSE_Loss: 14.5024 - val_loss: 23.7629 - val_MSE_Loss: 17.3916 - lr: 1.0000e-05\n",
      "Epoch 16/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 19.5432 - MSE_Loss: 14.2497 - val_loss: 23.5246 - val_MSE_Loss: 17.1536 - lr: 1.0000e-05\n",
      "Epoch 17/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 19.5646 - MSE_Loss: 14.0848 - val_loss: 23.3376 - val_MSE_Loss: 16.9700 - lr: 1.0000e-05\n",
      "Epoch 18/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 19.2737 - MSE_Loss: 13.8637 - val_loss: 23.1744 - val_MSE_Loss: 16.8279 - lr: 1.0000e-05\n",
      "Epoch 19/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 19.0378 - MSE_Loss: 13.6890 - val_loss: 23.0138 - val_MSE_Loss: 16.6721 - lr: 1.0000e-05\n",
      "Epoch 20/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 18.7697 - MSE_Loss: 13.5126 - val_loss: 22.8985 - val_MSE_Loss: 16.5485 - lr: 1.0000e-05\n",
      "Epoch 21/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 18.7582 - MSE_Loss: 13.3604 - val_loss: 22.7518 - val_MSE_Loss: 16.3964 - lr: 1.0000e-05\n",
      "Epoch 22/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 18.5426 - MSE_Loss: 13.2365 - val_loss: 22.6323 - val_MSE_Loss: 16.2868 - lr: 1.0000e-05\n",
      "Epoch 23/300\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 18.4179 - MSE_Loss: 13.1417 - val_loss: 22.5191 - val_MSE_Loss: 16.1821 - lr: 1.0000e-05\n",
      "Epoch 24/300\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 18.3568 - MSE_Loss: 13.0155 - val_loss: 22.4028 - val_MSE_Loss: 16.0626 - lr: 1.0000e-05\n",
      "Epoch 25/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 18.0761 - MSE_Loss: 12.8752 - val_loss: 22.3246 - val_MSE_Loss: 15.9944 - lr: 1.0000e-05\n",
      "Epoch 26/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 18.1839 - MSE_Loss: 12.7814 - val_loss: 22.2709 - val_MSE_Loss: 15.9431 - lr: 1.0000e-05\n",
      "Epoch 27/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 18.0257 - MSE_Loss: 12.7020 - val_loss: 22.1697 - val_MSE_Loss: 15.8444 - lr: 1.0000e-05\n",
      "Epoch 28/300\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 17.8637 - MSE_Loss: 12.6149 - val_loss: 22.1260 - val_MSE_Loss: 15.7950 - lr: 1.0000e-05\n",
      "Epoch 29/300\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 17.9597 - MSE_Loss: 12.5337 - val_loss: 22.0629 - val_MSE_Loss: 15.7364 - lr: 1.0000e-05\n",
      "Epoch 30/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 8ms/step - loss: 17.5740 - MSE_Loss: 12.4702 - val_loss: 21.9938 - val_MSE_Loss: 15.6680 - lr: 1.0000e-05\n",
      "Epoch 31/300\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 17.8165 - MSE_Loss: 12.4943 - val_loss: 21.9764 - val_MSE_Loss: 15.6469 - lr: 1.0000e-05\n",
      "Epoch 32/300\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 17.7673 - MSE_Loss: 12.3821 - val_loss: 21.9326 - val_MSE_Loss: 15.5962 - lr: 1.0000e-05\n",
      "Epoch 33/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 17.5462 - MSE_Loss: 12.2905 - val_loss: 21.8936 - val_MSE_Loss: 15.5735 - lr: 1.0000e-05\n",
      "Epoch 34/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 17.5124 - MSE_Loss: 12.2316 - val_loss: 21.8549 - val_MSE_Loss: 15.5328 - lr: 1.0000e-05\n",
      "Epoch 35/300\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 17.5533 - MSE_Loss: 12.2083 - val_loss: 21.8298 - val_MSE_Loss: 15.5081 - lr: 1.0000e-05\n",
      "Epoch 36/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 17.4564 - MSE_Loss: 12.1541 - val_loss: 21.7632 - val_MSE_Loss: 15.4424 - lr: 1.0000e-05\n",
      "Epoch 37/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 17.4884 - MSE_Loss: 12.1165 - val_loss: 21.7054 - val_MSE_Loss: 15.3856 - lr: 1.0000e-05\n",
      "Epoch 38/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 17.2434 - MSE_Loss: 12.0770 - val_loss: 21.6927 - val_MSE_Loss: 15.3758 - lr: 1.0000e-05\n",
      "Epoch 39/300\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 17.4040 - MSE_Loss: 12.0220 - val_loss: 21.7183 - val_MSE_Loss: 15.3932 - lr: 1.0000e-05\n",
      "Epoch 40/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 17.1814 - MSE_Loss: 12.0163 - val_loss: 21.6516 - val_MSE_Loss: 15.3361 - lr: 1.0000e-05\n",
      "Epoch 41/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 17.2083 - MSE_Loss: 11.9518 - val_loss: 21.5848 - val_MSE_Loss: 15.2686 - lr: 1.0000e-05\n",
      "Epoch 42/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 17.1848 - MSE_Loss: 11.9221 - val_loss: 21.5664 - val_MSE_Loss: 15.2543 - lr: 1.0000e-05\n",
      "Epoch 43/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 17.0921 - MSE_Loss: 11.9026 - val_loss: 21.5616 - val_MSE_Loss: 15.2481 - lr: 1.0000e-05\n",
      "Epoch 44/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 17.1343 - MSE_Loss: 11.8469 - val_loss: 21.5501 - val_MSE_Loss: 15.2415 - lr: 1.0000e-05\n",
      "Epoch 45/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.9547 - MSE_Loss: 11.8317 - val_loss: 21.4692 - val_MSE_Loss: 15.1657 - lr: 1.0000e-05\n",
      "Epoch 46/300\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.8758 - MSE_Loss: 11.8319 - val_loss: 21.4446 - val_MSE_Loss: 15.1410 - lr: 1.0000e-05\n",
      "Epoch 47/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.9848 - MSE_Loss: 11.7863 - val_loss: 21.4381 - val_MSE_Loss: 15.1297 - lr: 1.0000e-05\n",
      "Epoch 48/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 17.0789 - MSE_Loss: 11.7835 - val_loss: 21.4195 - val_MSE_Loss: 15.1203 - lr: 1.0000e-05\n",
      "Epoch 49/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 17.0107 - MSE_Loss: 11.7138 - val_loss: 21.3870 - val_MSE_Loss: 15.0876 - lr: 1.0000e-05\n",
      "Epoch 50/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.9033 - MSE_Loss: 11.6952 - val_loss: 21.3595 - val_MSE_Loss: 15.0663 - lr: 1.0000e-05\n",
      "Epoch 51/300\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 17.5365 - MSE_Loss: 12.8413\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 17.1288 - MSE_Loss: 11.6823 - val_loss: 21.3290 - val_MSE_Loss: 15.0413 - lr: 1.0000e-05\n",
      "Epoch 52/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.9444 - MSE_Loss: 11.6463 - val_loss: 21.3479 - val_MSE_Loss: 15.0613 - lr: 5.0000e-06\n",
      "Epoch 53/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7295 - MSE_Loss: 11.6319 - val_loss: 21.3129 - val_MSE_Loss: 15.0289 - lr: 5.0000e-06\n",
      "Epoch 54/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.9203 - MSE_Loss: 11.6139 - val_loss: 21.2930 - val_MSE_Loss: 15.0059 - lr: 5.0000e-06\n",
      "Epoch 55/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 16.8358 - MSE_Loss: 11.5937 - val_loss: 21.2703 - val_MSE_Loss: 14.9879 - lr: 5.0000e-06\n",
      "Epoch 56/300\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 16.8555 - MSE_Loss: 11.5843 - val_loss: 21.2703 - val_MSE_Loss: 14.9859 - lr: 5.0000e-06\n",
      "Epoch 57/300\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 16.5442 - MSE_Loss: 11.5745 - val_loss: 21.2384 - val_MSE_Loss: 14.9576 - lr: 5.0000e-06\n",
      "Epoch 58/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.7430 - MSE_Loss: 11.5654 - val_loss: 21.2117 - val_MSE_Loss: 14.9356 - lr: 5.0000e-06\n",
      "Epoch 59/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.5826 - MSE_Loss: 11.5645 - val_loss: 21.2002 - val_MSE_Loss: 14.9279 - lr: 5.0000e-06\n",
      "Epoch 60/300\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 16.5951 - MSE_Loss: 11.5549 - val_loss: 21.1560 - val_MSE_Loss: 14.8877 - lr: 5.0000e-06\n",
      "Epoch 61/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7813 - MSE_Loss: 11.5371 - val_loss: 21.1706 - val_MSE_Loss: 14.9043 - lr: 5.0000e-06\n",
      "Epoch 62/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 22.0846 - MSE_Loss: 17.3903\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.8701 - MSE_Loss: 11.5241 - val_loss: 21.1702 - val_MSE_Loss: 14.9016 - lr: 5.0000e-06\n",
      "Epoch 63/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.4929 - MSE_Loss: 11.4943 - val_loss: 21.1568 - val_MSE_Loss: 14.8894 - lr: 2.5000e-06\n",
      "Epoch 64/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7742 - MSE_Loss: 11.4914 - val_loss: 21.1380 - val_MSE_Loss: 14.8723 - lr: 2.5000e-06\n",
      "Epoch 65/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.7936 - MSE_Loss: 11.4936 - val_loss: 21.1208 - val_MSE_Loss: 14.8571 - lr: 2.5000e-06\n",
      "Epoch 66/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.7652 - MSE_Loss: 11.4853 - val_loss: 21.1170 - val_MSE_Loss: 14.8526 - lr: 2.5000e-06\n",
      "Epoch 67/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.5906 - MSE_Loss: 11.4801 - val_loss: 21.1334 - val_MSE_Loss: 14.8690 - lr: 2.5000e-06\n",
      "Epoch 68/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.8224 - MSE_Loss: 12.1284\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6036 - MSE_Loss: 11.4750 - val_loss: 21.1128 - val_MSE_Loss: 14.8495 - lr: 2.5000e-06\n",
      "Epoch 69/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7434 - MSE_Loss: 11.4639 - val_loss: 21.1059 - val_MSE_Loss: 14.8454 - lr: 1.2500e-06\n",
      "Epoch 70/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.5585 - MSE_Loss: 11.4551 - val_loss: 21.0963 - val_MSE_Loss: 14.8371 - lr: 1.2500e-06\n",
      "Epoch 71/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 16.6828 - MSE_Loss: 11.4576 - val_loss: 21.0844 - val_MSE_Loss: 14.8259 - lr: 1.2500e-06\n",
      "Epoch 72/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.6561 - MSE_Loss: 11.4502 - val_loss: 21.0878 - val_MSE_Loss: 14.8293 - lr: 1.2500e-06\n",
      "Epoch 73/300\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 16.8360 - MSE_Loss: 12.1420\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.5917 - MSE_Loss: 11.4477 - val_loss: 21.0900 - val_MSE_Loss: 14.8317 - lr: 1.2500e-06\n",
      "Epoch 74/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7135 - MSE_Loss: 11.4436 - val_loss: 21.0846 - val_MSE_Loss: 14.8268 - lr: 6.2500e-07\n",
      "Epoch 75/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7323 - MSE_Loss: 11.4429 - val_loss: 21.0784 - val_MSE_Loss: 14.8207 - lr: 6.2500e-07\n",
      "Epoch 76/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 16.6153 - MSE_Loss: 11.4398 - val_loss: 21.0804 - val_MSE_Loss: 14.8231 - lr: 6.2500e-07\n",
      "Epoch 77/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 16.5529 - MSE_Loss: 11.4384 - val_loss: 21.0752 - val_MSE_Loss: 14.8184 - lr: 6.2500e-07\n",
      "Epoch 78/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.3273 - MSE_Loss: 14.6335\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.5726 - MSE_Loss: 11.4392 - val_loss: 21.0734 - val_MSE_Loss: 14.8166 - lr: 6.2500e-07\n",
      "Epoch 79/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7317 - MSE_Loss: 11.4345 - val_loss: 21.0724 - val_MSE_Loss: 14.8158 - lr: 3.1250e-07\n",
      "Epoch 80/300\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 16.7515 - MSE_Loss: 11.4330 - val_loss: 21.0705 - val_MSE_Loss: 14.8141 - lr: 3.1250e-07\n",
      "Epoch 81/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.7549 - MSE_Loss: 11.4335 - val_loss: 21.0697 - val_MSE_Loss: 14.8137 - lr: 3.1250e-07\n",
      "Epoch 82/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 16.7802 - MSE_Loss: 11.4326 - val_loss: 21.0684 - val_MSE_Loss: 14.8124 - lr: 3.1250e-07\n",
      "Epoch 83/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.1735 - MSE_Loss: 14.4796\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.8080 - MSE_Loss: 11.4319 - val_loss: 21.0660 - val_MSE_Loss: 14.8103 - lr: 3.1250e-07\n",
      "Epoch 84/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7046 - MSE_Loss: 11.4303 - val_loss: 21.0668 - val_MSE_Loss: 14.8111 - lr: 1.5625e-07\n",
      "Epoch 85/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 16.7199 - MSE_Loss: 11.4297 - val_loss: 21.0658 - val_MSE_Loss: 14.8101 - lr: 1.5625e-07\n",
      "Epoch 86/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 16.5479 - MSE_Loss: 11.4291 - val_loss: 21.0658 - val_MSE_Loss: 14.8101 - lr: 1.5625e-07\n",
      "Epoch 87/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.4284 - MSE_Loss: 11.4288 - val_loss: 21.0656 - val_MSE_Loss: 14.8099 - lr: 1.5625e-07\n",
      "Epoch 88/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.6511 - MSE_Loss: 11.4287 - val_loss: 21.0645 - val_MSE_Loss: 14.8089 - lr: 1.5625e-07\n",
      "Epoch 89/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.7688 - MSE_Loss: 11.4282 - val_loss: 21.0635 - val_MSE_Loss: 14.8082 - lr: 1.5625e-07\n",
      "Epoch 90/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 16.7088 - MSE_Loss: 11.4276 - val_loss: 21.0626 - val_MSE_Loss: 14.8073 - lr: 1.5625e-07\n",
      "Epoch 91/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 16.7318 - MSE_Loss: 11.4279 - val_loss: 21.0623 - val_MSE_Loss: 14.8070 - lr: 1.5625e-07\n",
      "Epoch 92/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.5316 - MSE_Loss: 12.8378\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7316 - MSE_Loss: 11.4276 - val_loss: 21.0615 - val_MSE_Loss: 14.8063 - lr: 1.5625e-07\n",
      "Epoch 93/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 16.7417 - MSE_Loss: 11.4267 - val_loss: 21.0613 - val_MSE_Loss: 14.8061 - lr: 7.8125e-08\n",
      "Epoch 94/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6846 - MSE_Loss: 11.4264 - val_loss: 21.0613 - val_MSE_Loss: 14.8062 - lr: 7.8125e-08\n",
      "Epoch 95/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6613 - MSE_Loss: 11.4266 - val_loss: 21.0614 - val_MSE_Loss: 14.8063 - lr: 7.8125e-08\n",
      "Epoch 96/300\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 16.6955 - MSE_Loss: 11.4260 - val_loss: 21.0612 - val_MSE_Loss: 14.8061 - lr: 7.8125e-08\n",
      "Epoch 97/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.4204 - MSE_Loss: 10.7266\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-08.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.8524 - MSE_Loss: 11.4262 - val_loss: 21.0605 - val_MSE_Loss: 14.8054 - lr: 7.8125e-08\n",
      "Epoch 98/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 16.6844 - MSE_Loss: 11.4256 - val_loss: 21.0604 - val_MSE_Loss: 14.8053 - lr: 3.9062e-08\n",
      "Epoch 99/300\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 16.5453 - MSE_Loss: 11.4255 - val_loss: 21.0601 - val_MSE_Loss: 14.8050 - lr: 3.9062e-08\n",
      "Epoch 100/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.5886 - MSE_Loss: 11.4256 - val_loss: 21.0602 - val_MSE_Loss: 14.8052 - lr: 3.9062e-08\n",
      "Epoch 101/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.4053 - MSE_Loss: 11.4253 - val_loss: 21.0599 - val_MSE_Loss: 14.8049 - lr: 3.9062e-08\n",
      "Epoch 102/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.6680 - MSE_Loss: 11.4252 - val_loss: 21.0597 - val_MSE_Loss: 14.8047 - lr: 3.9062e-08\n",
      "Epoch 103/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.5837 - MSE_Loss: 11.4252 - val_loss: 21.0597 - val_MSE_Loss: 14.8048 - lr: 3.9062e-08\n",
      "Epoch 104/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6867 - MSE_Loss: 11.4252 - val_loss: 21.0594 - val_MSE_Loss: 14.8045 - lr: 3.9062e-08\n",
      "Epoch 105/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6965 - MSE_Loss: 11.4252 - val_loss: 21.0592 - val_MSE_Loss: 14.8043 - lr: 3.9062e-08\n",
      "Epoch 106/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.1856 - MSE_Loss: 12.4918\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-08.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7734 - MSE_Loss: 11.4250 - val_loss: 21.0593 - val_MSE_Loss: 14.8043 - lr: 3.9062e-08\n",
      "Epoch 107/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.6340 - MSE_Loss: 11.4248 - val_loss: 21.0592 - val_MSE_Loss: 14.8042 - lr: 1.9531e-08\n",
      "Epoch 108/300\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 16.7691 - MSE_Loss: 11.4248 - val_loss: 21.0593 - val_MSE_Loss: 14.8044 - lr: 1.9531e-08\n",
      "Epoch 109/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.4849 - MSE_Loss: 11.4247 - val_loss: 21.0592 - val_MSE_Loss: 14.8043 - lr: 1.9531e-08\n",
      "Epoch 110/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 16.7412 - MSE_Loss: 11.4246 - val_loss: 21.0590 - val_MSE_Loss: 14.8041 - lr: 1.9531e-08\n",
      "Epoch 111/300\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 16.5068 - MSE_Loss: 11.8130\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-09.\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 16.6054 - MSE_Loss: 11.4245 - val_loss: 21.0589 - val_MSE_Loss: 14.8040 - lr: 1.9531e-08\n",
      "Epoch 112/300\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 16.7306 - MSE_Loss: 11.4245 - val_loss: 21.0589 - val_MSE_Loss: 14.8040 - lr: 9.7656e-09\n",
      "Epoch 113/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.6426 - MSE_Loss: 11.4244 - val_loss: 21.0589 - val_MSE_Loss: 14.8040 - lr: 9.7656e-09\n",
      "Epoch 114/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 16.6617 - MSE_Loss: 11.4245 - val_loss: 21.0589 - val_MSE_Loss: 14.8040 - lr: 9.7656e-09\n",
      "Epoch 115/300\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 16.5241 - MSE_Loss: 11.4244 - val_loss: 21.0589 - val_MSE_Loss: 14.8040 - lr: 9.7656e-09\n",
      "Epoch 116/300\n",
      " 7/10 [====================>.........] - ETA: 0s - loss: 17.1491 - MSE_Loss: 12.4553\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-09.\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 16.6542 - MSE_Loss: 11.4244 - val_loss: 21.0588 - val_MSE_Loss: 14.8039 - lr: 9.7656e-09\n",
      "Epoch 117/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.6345 - MSE_Loss: 11.4244 - val_loss: 21.0588 - val_MSE_Loss: 14.8039 - lr: 4.8828e-09\n",
      "Epoch 118/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.7866 - MSE_Loss: 11.4244 - val_loss: 21.0588 - val_MSE_Loss: 14.8039 - lr: 4.8828e-09\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 7ms/step - loss: 16.4710 - MSE_Loss: 11.4244 - val_loss: 21.0588 - val_MSE_Loss: 14.8039 - lr: 4.8828e-09\n",
      "Epoch 120/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6984 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8039 - lr: 4.8828e-09\n",
      "Epoch 121/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.3267 - MSE_Loss: 11.6329\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-09.\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.6778 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8039 - lr: 4.8828e-09\n",
      "Epoch 122/300\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 16.6635 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8039 - lr: 2.4414e-09\n",
      "Epoch 123/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.7911 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8039 - lr: 2.4414e-09\n",
      "Epoch 124/300\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.4551 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8039 - lr: 2.4414e-09\n",
      "Epoch 125/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.6928 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8039 - lr: 2.4414e-09\n",
      "Epoch 126/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.3288 - MSE_Loss: 13.6350\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-09.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6662 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 2.4414e-09\n",
      "Epoch 127/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 16.6756 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.2207e-09\n",
      "Epoch 128/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 16.7626 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.2207e-09\n",
      "Epoch 129/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.6067 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.2207e-09\n",
      "Epoch 130/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.7086 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.2207e-09\n",
      "Epoch 131/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.5182 - MSE_Loss: 11.8244\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-10.\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.4747 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.2207e-09\n",
      "Epoch 132/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6539 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 6.1035e-10\n",
      "Epoch 133/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.4931 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 6.1035e-10\n",
      "Epoch 134/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.7122 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 6.1035e-10\n",
      "Epoch 135/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.6586 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 6.1035e-10\n",
      "Epoch 136/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.9810 - MSE_Loss: 11.2872\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-10.\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.7458 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 6.1035e-10\n",
      "Epoch 137/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.6367 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 3.0518e-10\n",
      "Epoch 138/300\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 16.7492 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 3.0518e-10\n",
      "Epoch 139/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 16.6394 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 3.0518e-10\n",
      "Epoch 140/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 16.5950 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 3.0518e-10\n",
      "Epoch 141/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.1145 - MSE_Loss: 12.4207\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-10.\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.6795 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 3.0518e-10\n",
      "Epoch 142/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.7195 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.5259e-10\n",
      "Epoch 143/300\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 16.5441 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.5259e-10\n",
      "Epoch 144/300\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 16.6824 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.5259e-10\n",
      "Epoch 145/300\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 16.6293 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.5259e-10\n",
      "Epoch 146/300\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 16.7808 - MSE_Loss: 12.0870\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-11.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.6836 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.5259e-10\n",
      "Epoch 147/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.5503 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 7.6294e-11\n",
      "Epoch 148/300\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 16.8440 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 7.6294e-11\n",
      "Epoch 149/300\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 16.5667 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 7.6294e-11\n",
      "Epoch 150/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6728 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 7.6294e-11\n",
      "Epoch 151/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.3281 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 7.6294e-11\n",
      "Epoch 152/300\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 16.6055 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 7.6294e-11\n",
      "Epoch 153/300\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 16.5972 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 7.6294e-11\n",
      "Epoch 154/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.5554 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 7.6294e-11\n",
      "Epoch 155/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.6707 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 7.6294e-11\n",
      "Epoch 156/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.4456 - MSE_Loss: 8.7517\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-11.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6662 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 7.6294e-11\n",
      "Epoch 157/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.6351 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 3.8147e-11\n",
      "Epoch 158/300\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 16.6428 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 3.8147e-11\n",
      "Epoch 159/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.7645 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 3.8147e-11\n",
      "Epoch 160/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.6848 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 3.8147e-11\n",
      "Epoch 161/300\n",
      "10/10 [==============================] - ETA: 0s - loss: 16.6789 - MSE_Loss: 11.4243\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-11.\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 16.6789 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 3.8147e-11\n",
      "Epoch 162/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 9ms/step - loss: 16.7579 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.9073e-11\n",
      "Epoch 163/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.6645 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.9073e-11\n",
      "Epoch 164/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.5390 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.9073e-11\n",
      "Epoch 165/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.7112 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.9073e-11\n",
      "Epoch 166/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.4718 - MSE_Loss: 11.7780\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-12.\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.7246 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.9073e-11\n",
      "Epoch 167/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.5767 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 9.5367e-12\n",
      "Epoch 168/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.7632 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 9.5367e-12\n",
      "Epoch 169/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.7412 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 9.5367e-12\n",
      "Epoch 170/300\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.5830 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 9.5367e-12\n",
      "Epoch 171/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.5558 - MSE_Loss: 9.8620\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-12.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6115 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 9.5367e-12\n",
      "Epoch 172/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6947 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 4.7684e-12\n",
      "Epoch 173/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.4638 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 4.7684e-12\n",
      "Epoch 174/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7121 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 4.7684e-12\n",
      "Epoch 175/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.4905 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 4.7684e-12\n",
      "Epoch 176/300\n",
      " 5/10 [==============>...............] - ETA: 0s - loss: 16.9805 - MSE_Loss: 12.2866\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-12.\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 16.5482 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 4.7684e-12\n",
      "Epoch 177/300\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.4951 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 2.3842e-12\n",
      "Epoch 178/300\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 16.5538 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 2.3842e-12\n",
      "Epoch 179/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.7635 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 2.3842e-12\n",
      "Epoch 180/300\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 16.6721 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 2.3842e-12\n",
      "Epoch 181/300\n",
      "10/10 [==============================] - ETA: 0s - loss: 16.6757 - MSE_Loss: 11.4243\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-12.\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.6757 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 2.3842e-12\n",
      "Epoch 182/300\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 16.7172 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.1921e-12\n",
      "Epoch 183/300\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 16.7114 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.1921e-12\n",
      "Epoch 184/300\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 16.6663 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.1921e-12\n",
      "Epoch 185/300\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.6550 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.1921e-12\n",
      "Epoch 186/300\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 17.1920 - MSE_Loss: 12.4981\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-13.\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.7509 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.1921e-12\n",
      "Epoch 187/300\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.6043 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 5.9605e-13\n",
      "Epoch 188/300\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 16.6498 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 5.9605e-13\n",
      "Epoch 189/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.4624 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 5.9605e-13\n",
      "Epoch 190/300\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.3803 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 5.9605e-13\n",
      "Epoch 191/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.4841 - MSE_Loss: 11.7903\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 2.9802321634825324e-13.\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 16.6555 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 5.9605e-13\n",
      "Epoch 192/300\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 16.6296 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 2.9802e-13\n",
      "Epoch 193/300\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.5130 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 2.9802e-13\n",
      "Epoch 194/300\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 16.7973 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 2.9802e-13\n",
      "Epoch 195/300\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 16.7010 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 2.9802e-13\n",
      "Epoch 196/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.9595 - MSE_Loss: 11.2657\n",
      "Epoch 196: ReduceLROnPlateau reducing learning rate to 1.4901160817412662e-13.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.6727 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 2.9802e-13\n",
      "Epoch 197/300\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 16.7792 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.4901e-13\n",
      "Epoch 198/300\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 16.7986 - MSE_Loss: 11.4243 - val_loss: 21.0587 - val_MSE_Loss: 14.8038 - lr: 1.4901e-13\n",
      "***************************** training_time is:  20.8580379486084\n",
      "Actual ATE: 3.8536534 \n",
      "\n",
      "\n",
      "Estimated ATE: 3.684378 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py --model tarnet --dataset IHDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "eae5a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.load('./save/pred_result/pred_result_tarnet_IHDP.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cc674c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "96adb66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "pite = sns.kdeplot(cate_pred.squeeze(), palette = 'blue', label = 'Predicted ITE')\n",
    "tite = sns.kdeplot(cate_true.squeeze(), palette = 'red', label = 'True ITE')\n",
    "plt.title(\"True ITE vs. Predicted ITE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
